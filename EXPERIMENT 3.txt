import random
import math

# Sigmoid activation function
def sigmoid(x):
    return 1 / (1 + math.exp(-x))

# Derivative of sigmoid
def sigmoid_derivative(x):
    return x * (1 - x)

# Training dataset (XOR problem)
# Inputs and expected outputs
training_data = [
    ([0, 0], [0]),
    ([0, 1], [1]),
    ([1, 0], [1]),
    ([1, 1], [0])
]

# Initialize weights randomly
input_layer_neurons = 2
hidden_layer_neurons = 2
output_neurons = 1

# Weights
weights_input_hidden = [[random.uniform(-1, 1) for _ in range(hidden_layer_neurons)]
                        for _ in range(input_layer_neurons)]

weights_hidden_output = [[random.uniform(-1, 1)]
                         for _ in range(hidden_layer_neurons)]

# Biases
bias_hidden = [random.uniform(-1, 1) for _ in range(hidden_layer_neurons)]
bias_output = random.uniform(-1, 1)

learning_rate = 0.5
epochs = 5000

# Training using Backpropagation
for epoch in range(epochs):
    for inputs, target in training_data:

        # ---- Forward Propagation ----
        hidden_layer_output = []
        for i in range(hidden_layer_neurons):
            activation = bias_hidden[i]
            for j in range(input_layer_neurons):
                activation += inputs[j] * weights_input_hidden[j][i]
            hidden_layer_output.append(sigmoid(activation))

        output_activation = bias_output
        for i in range(hidden_layer_neurons):
            output_activation += hidden_layer_output[i] * weights_hidden_output[i][0]
        predicted_output = sigmoid(output_activation)

        # ---- Backpropagation ----
        output_error = target[0] - predicted_output
        output_delta = output_error * sigmoid_derivative(predicted_output)

        hidden_deltas = []
        for i in range(hidden_layer_neurons):
            error = output_delta * weights_hidden_output[i][0]
            hidden_deltas.append(error * sigmoid_derivative(hidden_layer_output[i]))

        # Update weights and biases
        for i in range(hidden_layer_neurons):
            weights_hidden_output[i][0] += learning_rate * output_delta * hidden_layer_output[i]

        bias_output += learning_rate * output_delta

        for i in range(input_layer_neurons):
            for j in range(hidden_layer_neurons):
                weights_input_hidden[i][j] += learning_rate * hidden_deltas[j] * inputs[i]

        for i in range(hidden_layer_neurons):
            bias_hidden[i] += learning_rate * hidden_deltas[i]

# ---- Testing ----
print("Testing Neural Network after Training:\n")
for inputs, target in training_data:
    hidden_layer_output = []
    for i in range(hidden_layer_neurons):
        activation = bias_hidden[i]
        for j in range(input_layer_neurons):
            activation += inputs[j] * weights_input_hidden[j][i]
        hidden_layer_output.append(sigmoid(activation))

    output_activation = bias_output
    for i in range(hidden_layer_neurons):
        output_activation += hidden_layer_output[i] * weights_hidden_output[i][0]
    predicted_output = sigmoid(output_activation)

    print("Input:", inputs, "Predicted Output:", round(predicted_output, 3), "Expected:", target[0])
